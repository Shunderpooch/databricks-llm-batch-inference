# llm-batch-inference-structured-outputs
Simple end-to-end demo that shows how to run batch inference on PT endpoint and fine-tunes LLM to generate structured outputs
